{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Detecting the difficulty level of French texts](https://www.kaggle.com/c/detecting-the-difficulty-level-of-french-texts/overview/evaluation)\n",
    "## Model improvement\n",
    "---\n",
    "In this notebook, we will try differents methods to improve the accuracy of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import string\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "np.random.state = 0\n",
    "\n",
    "\n",
    "def evaluate(y_true, pred):\n",
    "    \"\"\"\n",
    "    Calculate the models performance metrics. \n",
    "    Since it is a multi-class classification, we take the weighted average \n",
    "    for the metrics that are calculated for each class.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    report = {\n",
    "      'accuracy':accuracy_score(y_true, pred),\n",
    "      'recall':recall_score(y_true, pred, average='weighted'),\n",
    "      'precision':precision_score(y_true, pred, average='weighted'),\n",
    "      'f1_score':f1_score(y_true, pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, pred, model):  \n",
    "    \"\"\"\n",
    "    A function to plot the models confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    cf_matrix = confusion_matrix(y_test, pred)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix,\n",
    "                              display_labels=model.classes_)\n",
    "\n",
    "    disp.plot()\n",
    "\n",
    "\n",
    "sp = spacy.load('fr_core_news_md')\n",
    "\n",
    "# Import stopwords from spacy french language\n",
    "stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "# Import punctations characters\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence difficulty\n",
       "0   0  Les coûts kilométriques réels peuvent diverger...         C1\n",
       "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1\n",
       "2   2  Le test de niveau en français est sur le site ...         A1\n",
       "3   3           Est-ce que ton mari est aussi de Boston?         A1\n",
       "4   4  Dans les écoles de commerce, dans les couloirs...         B1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/LaCrazyTomato/Group-Project-DM-ML-2021/main/data/training_data.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['le', 'test', 'de', 'niveau', 'en', 'françai', 'est', 'sur', 'le', 'site', 'internet', 'de', \"l'école\", '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define cleaning function\n",
    "def nltk_tokenizer(doc):\n",
    "    \n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    # Tokenize and remove white spaces (strip)\n",
    "    doc = word_tokenize(doc)\n",
    "    doc = [word.lower().strip() for word in doc]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    doc = [stemmer.stem(word) for word in doc]\n",
    "    \n",
    "    lemma = WordNetLemmatizer()\n",
    "    doc = [lemma.lemmatize(word) for word in doc]\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "print(nltk_tokenizer(df.loc[2, 'sentence']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA\n",
    "We will first try PCA to reduce our models' dimensionnality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer with optimal parameters found previously\n",
    "vectorizer = TfidfVectorizer(tokenizer=nltk_tokenizer, \n",
    "                               ngram_range=(1, 6),\n",
    "                               analyzer='char',\n",
    "                            min_df=2,\n",
    "                            max_df=0.7,\n",
    "                            norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 111784)\n"
     ]
    }
   ],
   "source": [
    "X = df['sentence']\n",
    "y = df['difficulty']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0, \n",
    "                                                    stratify=y)\n",
    "\n",
    "# We nee to transform the features, before apply PCA on it\n",
    "X_train_vec = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_vec = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "\n",
    "# Then, we scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_vec = scaler.fit_transform(X_train_vec)\n",
    "X_test_vec = scaler.transform(X_test_vec)\n",
    "\n",
    "\n",
    "print(X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after PCA:  (3840, 2978)\n",
      "Number of components:  2978\n",
      "Explained variance ratio:  0.9500066963357994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Define PCA (we want 95% of explained variance)\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Example on X_train_vec\n",
    "X_train_vec_pca = pca.fit_transform(X_train_vec)\n",
    "X_test_vec_pca = pca.transform(X_test_vec)\n",
    "\n",
    "print('Shape after PCA: ', X_train_vec_pca.shape)\n",
    "print('Number of components: ', pca.n_components_)\n",
    "print('Explained variance ratio: ', sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Logistic Regression\n",
    "### 1.1.1 Remainder : accuracy from previous step -> 50.31 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5041666666666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(max_iter=10_000,\n",
    "                          penalty='l2',\n",
    "                          solver='lbfgs')\n",
    "\n",
    "\n",
    "# We don't need a pipeline anymore since we already applied vectorizer\n",
    "lr_model.fit(X_train_vec_pca, y_train)\n",
    "\n",
    "lr_model.score(X_test_vec_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did not improve a lot.. Let's try only with the scaled values (no PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5125"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.fit(X_train_vec, y_train)\n",
    "\n",
    "lr_model.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks way better without PCA.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest\n",
    "### 2.1 Remainder : accuracy from previous approach -> 46.15 %\n",
    "With PCA :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.215625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest_model = RandomForestClassifier(max_depth=40,\n",
    "                                            n_estimators=80,\n",
    "                                           criterion='gini',\n",
    "                                           max_features='sqrt')\n",
    "\n",
    "\n",
    "# We don't need a pipeline anymore since we already applied vectorizer\n",
    "randomForest_model.fit(X_train_vec_pca, y_train)\n",
    "\n",
    "randomForest_model.score(X_test_vec_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scaler only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.425"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest_model.fit(X_train_vec, y_train)\n",
    "\n",
    "randomForest_model.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling or PCA doesn't seem to improve accuracy. We will keep only parameters found in previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ridge classifier\n",
    "### 3.1 Remainder : accuracy from previous approach -> 50.42 %\n",
    "With PCA :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48333333333333334"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model = RidgeClassifier(random_state=0, \n",
    "                        max_iter=10_000, \n",
    "                        alpha=1.2,\n",
    "                        solver='auto')\n",
    "\n",
    "\n",
    "# We don't need a pipeline anymore since we already applied vectorizer\n",
    "ridge_model.fit(X_train_vec_pca, y_train)\n",
    "\n",
    "ridge_model.score(X_test_vec_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scaler only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45729166666666665"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model.fit(X_train_vec, y_train)\n",
    "\n",
    "ridge_model.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without PCA or scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5041666666666667"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_pipe = Pipeline([('vectorizer', vectorizer),                 \n",
    "                 ('classifier', ridge_model)])\n",
    "\n",
    "\n",
    "ridge_pipe.fit(X_train, y_train)\n",
    "\n",
    "ridge_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perceptron classifier\n",
    "### 4.1 Remainder : accuracy from previous approach -> 46.77 %\n",
    "With PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.475"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_model = Perceptron()\n",
    "\n",
    "\n",
    "perceptron_model.fit(X_train_vec_pca, y_train)\n",
    "\n",
    "perceptron_model.score(X_test_vec_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scaling only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.459375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_model.fit(X_train_vec, y_train)\n",
    "\n",
    "perceptron_model.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PCA, we managed to improve accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ensemble method\n",
    "Another step to improve our prediction accuracy is to create an ensemble of models. StackingClassifier module from sklearn allows us to do that, very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "\n",
    "# We didn't find how to implement a scaler with a vectorizer in a pipeline.. Therefore, we will only use \n",
    "\n",
    "## Logistic Regression pipe\n",
    "lr_model = LogisticRegression(max_iter=10_000,\n",
    "                          penalty='l2',\n",
    "                          solver='lbfgs')\n",
    "\n",
    "lr_pipe = Pipeline([\n",
    "                    ('vectorizer', vectorizer),\n",
    "                    (\"classifier\", lr_model)\n",
    "                   ])\n",
    "\n",
    "\n",
    "## Random Forest pipe\n",
    "randomForest_model = RandomForestClassifier(max_depth=40,\n",
    "                                            n_estimators=80,\n",
    "                                           criterion='gini',\n",
    "                                           max_features='sqrt')\n",
    "\n",
    "randomForest_pipe = Pipeline([\n",
    "                    ('vectorizer', vectorizer),\n",
    "                    (\"classifier\", randomForest_model)\n",
    "                   ])\n",
    "\n",
    "\n",
    "## Ridge classifier pipe\n",
    "ridge_model = RidgeClassifier(random_state=0, \n",
    "                        max_iter=10_000, \n",
    "                        alpha=1.2,\n",
    "                        solver='auto')\n",
    "\n",
    "ridge_pipe = Pipeline([\n",
    "                    ('vectorizer', vectorizer),\n",
    "                    (\"classifier\", ridge_model)\n",
    "                   ])\n",
    "\n",
    "# Perceptron classsifier pipe\n",
    "perceptron_model = Perceptron()\n",
    "\n",
    "perceptron_pipe = Pipeline([\n",
    "                    ('vectorizer', vectorizer),\n",
    "                    (\"classifier\", perceptron_model)\n",
    "                   ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-83e6ae10-3250-4eb6-acf8-47664e563dae {color: black;background-color: white;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae pre{padding: 0;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-toggleable {background-color: white;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-estimator:hover {background-color: #d4ebff;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-item {z-index: 1;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-parallel-item:only-child::after {width: 0;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-83e6ae10-3250-4eb6-acf8-47664e563dae div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-83e6ae10-3250-4eb6-acf8-47664e563dae\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f9dc036d-9da7-4d0b-8886-c34330685a21\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f9dc036d-9da7-4d0b-8886-c34330685a21\">StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(estimators=[('Random Forest',\n",
       "                                Pipeline(steps=[('vectorizer',\n",
       "                                                 TfidfVectorizer(analyzer='char',\n",
       "                                                                 max_df=0.7,\n",
       "                                                                 min_df=2,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              6),\n",
       "                                                                 tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)),\n",
       "                                                ('classifier',\n",
       "                                                 RandomForestClassifier(max_depth=40,\n",
       "                                                                        max_features='sqrt',\n",
       "                                                                        n_estimators=80))])),\n",
       "                               ('Logistic Regression',\n",
       "                                Pipeline(steps=[('vectorizer',\n",
       "                                                 Tfi...\n",
       "                                                ('classifier',\n",
       "                                                 LogisticRegression(max_iter=10000))])),\n",
       "                               ('Ridge Classifier',\n",
       "                                Pipeline(steps=[('vectorizer',\n",
       "                                                 TfidfVectorizer(analyzer='char',\n",
       "                                                                 max_df=0.7,\n",
       "                                                                 min_df=2,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              6),\n",
       "                                                                 tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)),\n",
       "                                                ('classifier',\n",
       "                                                 RidgeClassifier(alpha=1.2,\n",
       "                                                                 max_iter=10000,\n",
       "                                                                 random_state=0))]))],\n",
       "                   final_estimator=LogisticRegressionCV(max_iter=10000))</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Random Forest</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"676065f6-5eb3-4818-b85a-fc0002b21d42\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"676065f6-5eb3-4818-b85a-fc0002b21d42\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer='char', max_df=0.7, min_df=2, ngram_range=(1, 6),\n",
       "                tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"0a0139ee-41a7-4dee-a533-a2a346d023d7\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"0a0139ee-41a7-4dee-a533-a2a346d023d7\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=40, max_features='sqrt', n_estimators=80)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Logistic Regression</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cb0459ce-2d69-4aa6-b0b3-e9210932d247\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"cb0459ce-2d69-4aa6-b0b3-e9210932d247\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer='char', max_df=0.7, min_df=2, ngram_range=(1, 6),\n",
       "                tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cd75f1ba-ce97-4125-91fe-beb5cee6a21d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"cd75f1ba-ce97-4125-91fe-beb5cee6a21d\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Ridge Classifier</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5e979b18-b51a-4afc-bc82-98fc02223be1\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5e979b18-b51a-4afc-bc82-98fc02223be1\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer='char', max_df=0.7, min_df=2, ngram_range=(1, 6),\n",
       "                tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"81261d7e-8fc5-406b-991d-3e098ea94823\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"81261d7e-8fc5-406b-991d-3e098ea94823\">RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier(alpha=1.2, max_iter=10000, random_state=0)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"6aa9cf71-8185-43ad-9fa8-63f95c48dd05\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"6aa9cf71-8185-43ad-9fa8-63f95c48dd05\">LogisticRegressionCV</label><div class=\"sk-toggleable__content\"><pre>LogisticRegressionCV(max_iter=10000)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingClassifier(estimators=[('Random Forest',\n",
       "                                Pipeline(steps=[('vectorizer',\n",
       "                                                 TfidfVectorizer(analyzer='char',\n",
       "                                                                 max_df=0.7,\n",
       "                                                                 min_df=2,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              6),\n",
       "                                                                 tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)),\n",
       "                                                ('classifier',\n",
       "                                                 RandomForestClassifier(max_depth=40,\n",
       "                                                                        max_features='sqrt',\n",
       "                                                                        n_estimators=80))])),\n",
       "                               ('Logistic Regression',\n",
       "                                Pipeline(steps=[('vectorizer',\n",
       "                                                 Tfi...\n",
       "                                                ('classifier',\n",
       "                                                 LogisticRegression(max_iter=10000))])),\n",
       "                               ('Ridge Classifier',\n",
       "                                Pipeline(steps=[('vectorizer',\n",
       "                                                 TfidfVectorizer(analyzer='char',\n",
       "                                                                 max_df=0.7,\n",
       "                                                                 min_df=2,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              6),\n",
       "                                                                 tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)),\n",
       "                                                ('classifier',\n",
       "                                                 RidgeClassifier(alpha=1.2,\n",
       "                                                                 max_iter=10000,\n",
       "                                                                 random_state=0))]))],\n",
       "                   final_estimator=LogisticRegressionCV(max_iter=10000))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5541666666666667"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeCV\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "estimators = [\n",
    "    (\"Random Forest\", randomForest_pipe),\n",
    "    (\"Logistic Regression\", lr_pipe),\n",
    "    ('Ridge Classifier', ridge_pipe)\n",
    "]\n",
    "\n",
    "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=LogisticRegressionCV(max_iter=10_000))\n",
    "\n",
    "display(stacking_classifier)\n",
    "\n",
    "print(\"Accuracy:\")\n",
    "stacking_classifier.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the kaggle unlabeled set, this model achieved a 52.08 % score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-4543f252-036e-49f9-91a5-677bee7a1b76 {color: black;background-color: white;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 pre{padding: 0;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-toggleable {background-color: white;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-estimator:hover {background-color: #d4ebff;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-item {z-index: 1;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-parallel-item:only-child::after {width: 0;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-4543f252-036e-49f9-91a5-677bee7a1b76 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-4543f252-036e-49f9-91a5-677bee7a1b76\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"eaf4ab7f-cfaf-4b9d-8020-1d61e201bd8a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"eaf4ab7f-cfaf-4b9d-8020-1d61e201bd8a\">StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(estimators=[('Perceptron Classifier',\n",
       "                                Pipeline(steps=[('vectorizer',\n",
       "                                                 TfidfVectorizer(analyzer='char',\n",
       "                                                                 max_df=0.7,\n",
       "                                                                 min_df=2,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              6),\n",
       "                                                                 tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)),\n",
       "                                                ('classifier', Perceptron())])),\n",
       "                               ('Logistic Regression',\n",
       "                                Pipeline(steps=[('vectorizer',\n",
       "                                                 TfidfVectorizer(analyzer='char',\n",
       "                                                                 max_df=0.7,\n",
       "                                                                 min_df=2,\n",
       "                                                                 ngr...\n",
       "                                                ('classifier',\n",
       "                                                 LogisticRegression(max_iter=10000))])),\n",
       "                               ('Ridge Classifier',\n",
       "                                Pipeline(steps=[('vectorizer',\n",
       "                                                 TfidfVectorizer(analyzer='char',\n",
       "                                                                 max_df=0.7,\n",
       "                                                                 min_df=2,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              6),\n",
       "                                                                 tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)),\n",
       "                                                ('classifier',\n",
       "                                                 RidgeClassifier(alpha=1.2,\n",
       "                                                                 max_iter=10000,\n",
       "                                                                 random_state=0))]))],\n",
       "                   final_estimator=LogisticRegressionCV(max_iter=10000))</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Perceptron Classifier</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"fb9fd35f-cc35-48a3-b963-24f8995925db\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"fb9fd35f-cc35-48a3-b963-24f8995925db\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer='char', max_df=0.7, min_df=2, ngram_range=(1, 6),\n",
       "                tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"8b459683-3e5d-450c-88d7-b04181dcd83f\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"8b459683-3e5d-450c-88d7-b04181dcd83f\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Logistic Regression</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"72637deb-805d-4701-89cb-d84108d887df\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"72637deb-805d-4701-89cb-d84108d887df\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer='char', max_df=0.7, min_df=2, ngram_range=(1, 6),\n",
       "                tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cacb5bbb-afbe-4a15-9e2e-2ad8deb70fa2\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"cacb5bbb-afbe-4a15-9e2e-2ad8deb70fa2\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Ridge Classifier</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"8a897c59-3bb6-47cb-b2b8-b7e3ec9c7100\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"8a897c59-3bb6-47cb-b2b8-b7e3ec9c7100\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer='char', max_df=0.7, min_df=2, ngram_range=(1, 6),\n",
       "                tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"67a80d6e-5dd3-407a-8100-a8b632196190\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"67a80d6e-5dd3-407a-8100-a8b632196190\">RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier(alpha=1.2, max_iter=10000, random_state=0)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"7914e3a3-9687-4cf4-a051-973533ca1d0b\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"7914e3a3-9687-4cf4-a051-973533ca1d0b\">LogisticRegressionCV</label><div class=\"sk-toggleable__content\"><pre>LogisticRegressionCV(max_iter=10000)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingClassifier(estimators=[('Perceptron Classifier',\n",
       "                                Pipeline(steps=[('vectorizer',\n",
       "                                                 TfidfVectorizer(analyzer='char',\n",
       "                                                                 max_df=0.7,\n",
       "                                                                 min_df=2,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              6),\n",
       "                                                                 tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)),\n",
       "                                                ('classifier', Perceptron())])),\n",
       "                               ('Logistic Regression',\n",
       "                                Pipeline(steps=[('vectorizer',\n",
       "                                                 TfidfVectorizer(analyzer='char',\n",
       "                                                                 max_df=0.7,\n",
       "                                                                 min_df=2,\n",
       "                                                                 ngr...\n",
       "                                                ('classifier',\n",
       "                                                 LogisticRegression(max_iter=10000))])),\n",
       "                               ('Ridge Classifier',\n",
       "                                Pipeline(steps=[('vectorizer',\n",
       "                                                 TfidfVectorizer(analyzer='char',\n",
       "                                                                 max_df=0.7,\n",
       "                                                                 min_df=2,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              6),\n",
       "                                                                 tokenizer=<function nltk_tokenizer at 0x0000022BAD679708>)),\n",
       "                                                ('classifier',\n",
       "                                                 RidgeClassifier(alpha=1.2,\n",
       "                                                                 max_iter=10000,\n",
       "                                                                 random_state=0))]))],\n",
       "                   final_estimator=LogisticRegressionCV(max_iter=10000))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55625"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    (\"Perceptron Classifier\", perceptron_pipe),\n",
    "    (\"Logistic Regression\", lr_pipe),\n",
    "    ('Ridge Classifier', ridge_pipe)\n",
    "]\n",
    "\n",
    "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=LogisticRegressionCV(max_iter=10_000))\n",
    "\n",
    "display(stacking_classifier)\n",
    "\n",
    "print(\"Accuracy:\")\n",
    "stacking_classifier.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better with Perceptron instead of Random foret. On the kaggle unlabeled set, this model achieved a score of approximately 53 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we go any further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After thinking about how to further improve the model, we realized that the problem was probably with the vectorizer. The 4800 sentences of the training data are not enough to capture the whole French vocabulary. If the model sees a new word in a sentence it has to classify, it may react in a wrong way.\n",
    "\n",
    "For these reasons, we started looking for a pre-trained word embedding model. Mr. Vlachos had mentioned in class that there is a model called BERT that we could try. We did our research based on this advice and found a model called CAMEMBERT, which is based on the same vectorization technology as BERT but trained on the French language. \n",
    "\n",
    "We will see in the next notebook how we implemented this model and managed to reach 57% accuracy on unlabeled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](https://miro.medium.com/max/1200/1*E9NixJnfi8aVGU3ofiqQ8Q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
